{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Currency(BaseModel):\n",
    "    currency: str = Field(description=\"Currency code\")\n",
    "    amount: float = Field(description=\"Amount of the currency\")\n",
    "\n",
    "\n",
    "class CutoffScoreDetails(BaseModel):\n",
    "    year: int = Field(description=\"Year of the cutoff score\")\n",
    "    cutoff_score: float = Field(description=\"Cutoff score of the university\")\n",
    "\n",
    "class MajorDetails(BaseModel):\n",
    "    major_id: str = Field(description=\"ID of the major\")\n",
    "    major_name: str = Field(description=\"Name of the major\")\n",
    "    major_cutoff_details: list[CutoffScoreDetails] = Field(description=\"Cutoff score details of the major\")\n",
    "    subject_combinations: list[str] = Field(description=\"Subject combinations of the major\")\n",
    "    tuition_fee: float = Field(description=\"Tuition fee of the major per year\")\n",
    "    note: str = Field(description=\"Notes about the major\")\n",
    "\n",
    "class AdmissionDetails(BaseModel):\n",
    "    year: int = Field(description=\"Year of the application details\")\n",
    "    admission_target: int = Field(description=\"Admission target of the university\")\n",
    "    methods: list[str] = Field(description=\"Methods of admission\")\n",
    "\n",
    "class UniversityContact(BaseModel):\n",
    "    location: str = Field(description=\"Location of the university\")\n",
    "    phone: list[str] = Field(description=\"Phone number(s) of the university\")\n",
    "    website: str = Field(description=\"Website of the university\")\n",
    "    email: str = Field(description=\"Email of the university\")\n",
    "\n",
    "class University(BaseModel):\n",
    "    id: str = Field(description=\"ID of the university\")\n",
    "    name: str = Field(description=\"Name of the university\")\n",
    "    region: str = Field(description=\"Region where the university is located\")\n",
    "    contact: UniversityContact = Field(description=\"Contact details of the university\")\n",
    "    admission_details: AdmissionDetails = Field(description=\"Admission details of the university\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from selenium.common.exceptions import NoSuchWindowException\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[{asctime} - {levelname}]: {message}\", style=\"{\")\n",
    "logging.basicConfig(level=logging.WARNING, format=\"[{asctime} - {levelname}]: {message}\", style=\"{\")\n",
    "\n",
    "class VnExpressUniCrawler:\n",
    "    def __init__(self, progress_track: int, data_path: str) -> None:      \n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument(\"--window-size=1280,720\")\n",
    "        self.url = \"https://diemthi.vnexpress.net/tra-cuu-dai-hoc\"\n",
    "        self.driver = webdriver.Chrome(options=self.options)\n",
    "        self.uni_links = []\n",
    "        self.progress = progress_track\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def clear_progress(self):\n",
    "        self.progress = 0\n",
    "\n",
    "    def restart(self):\n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=self.options)\n",
    "\n",
    "            logging.info(f\"Restarting at Index {self.progress}: {self.uni_links[self.progress]}\")\n",
    "\n",
    "            for link in self.uni_links[self.progress:]:\n",
    "                self.driver.get(link)\n",
    "                self._crawl_uni_details()\n",
    "                self.progress = self.uni_links.index(link)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"[At {self.restart.__name__}: Couldn't restart.\\n{e}\")\n",
    "\n",
    "    def execute(self):\n",
    "        try:\n",
    "            # Get soup on initialize\n",
    "            self._crawl_main_page()\n",
    "            self.iterate_through_uni_links()\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[At {self.execute.__name__}]: {e}\")\n",
    "\n",
    "    \n",
    "    def iterate_through_uni_links(self, index: int = 0, **kwargs):\n",
    "        uni_links = kwargs.get(\"links\", None)\n",
    "        try:\n",
    "            if index != 0:\n",
    "                self.progress = index\n",
    "\n",
    "            if uni_links is not None:\n",
    "                self.uni_links = uni_links\n",
    "\n",
    "            for link in self.uni_links[index:]:\n",
    "                self.driver.get(link)\n",
    "                self._crawl_uni_details()\n",
    "                self.progress = self.uni_links.index(link)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[At {self.iterate_through_uni_links.__name__}]: {e}\")\n",
    "            self.driver.quit()\n",
    "            self.restart()\n",
    "        \n",
    "    def _crawl_main_page(self):\n",
    "        \n",
    "        self.driver.get(self.url)\n",
    "\n",
    "        loadmore_btn = self.driver.find_element(By.CLASS_NAME, \"btn_loadmore\")\n",
    "\n",
    "        while loadmore_btn:\n",
    "            try:\n",
    "                ActionChains(self.driver).move_to_element(loadmore_btn).click(loadmore_btn).pause(3).perform()\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                logging.info(\"Nothing left to load.\")\n",
    "\n",
    "                self.main_content =  self.driver.find_element(By.CLASS_NAME, \"main__content\").get_attribute(\"innerHTML\")\n",
    "                self.soup = BeautifulSoup(self.main_content, \"html.parser\")\n",
    "                \n",
    "                self._get_uni_href_list()\n",
    "\n",
    "                break\n",
    "\n",
    "    def _get_uni_href_list(self):\n",
    "        uni_list = self.soup.find('ul', {\"class\": \"lookup__results\"})\n",
    "\n",
    "        a_elements = uni_list.find_all('a')\n",
    "\n",
    "        for a in a_elements:\n",
    "            self.uni_links.append(str(\"https://diemthi.vnexpress.net\" + a['href']))\n",
    "\n",
    "        # Remove any duplicate links\n",
    "        self.uni_links = list(dict.fromkeys(self.uni_links))\n",
    "\n",
    "        with open(os.path.join(self.data_path, \"uni_links.txt\"), \"w\", encoding='utf-8') as f:\n",
    "            for link in self.uni_links:\n",
    "                f.write(link + \"\\n\")\n",
    "\n",
    "        logging.info(f\"Loading Completed. Found {len(self.uni_links)} universities.\")\n",
    "\n",
    "        return self.uni_links\n",
    "\n",
    "    def _extract_table_to_df(self, uni_id: str, region: str, year: int):\n",
    "        try:\n",
    "            table = self.driver.find_element(By.ID, \"detail_truong_other\").get_attribute(\"innerHTML\")\n",
    "\n",
    "            soup = BeautifulSoup(table, \"html.parser\")\n",
    "\n",
    "            df = pd.read_html(str(soup), index_col=0, header=0)[0]\n",
    "\n",
    "            df = df.dropna(how=\"all\")\n",
    "            df = df.iloc[:,:-1]\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            if table is None:\n",
    "                logging.warning(f\"[At {self._extract_table_to_df.__name__} | Index: {self.progress} | {region} - {uni_id} - {year}]:  No table found!\")\n",
    "                pass\n",
    "            print(e)\n",
    "    \n",
    "    def _save_dataframe(self, dataframe, file_name: str, uni_id: str, region: str, year: int):\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(os.path.join(self.data_path, region, uni_id)):\n",
    "                os.makedirs(os.path.join(self.data_path, region, uni_id))\n",
    "\n",
    "            path = os.path.join(self.data_path, region, uni_id, file_name)\n",
    "\n",
    "            logging.info(f\"Index {self.progress} | Saving '{region} - {uni_id} - {year}' admission scores to '{path}' ...\")\n",
    "            \n",
    "            dataframe.to_csv(path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            if dataframe is None:\n",
    "                logging.warning(f\"[At {self._save_dataframe.__name__} | Index {self.progress} | {region} - {uni_id} - {year}]: No dataframe found.\")\n",
    "                pass\n",
    "\n",
    "    def _get_uni_id_and_region(self):\n",
    "        try:\n",
    "            uni_id = self.driver.find_element(By.CLASS_NAME, \"university__header-code\").get_attribute(\"innerHTML\")\n",
    "\n",
    "            soup = BeautifulSoup(uni_id, \"html.parser\")\n",
    "\n",
    "            region_element = soup.find(\"strong\", {\"class\": \"university__header-location\"})\n",
    "\n",
    "            region = region_element.text\n",
    "\n",
    "            region_element.decompose()\n",
    "\n",
    "            uni_id = soup.text.strip()\n",
    "\n",
    "            uni_id = uni_id.replace(\"Mã trường: \", \"\")\n",
    "\n",
    "            uni_id = uni_id.replace(\" \", \"\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.driver.quit()\n",
    "            logging.error(f\"[At {self._get_uni_id_and_region.__name__}]: Failed to extract university ID and region.\")\n",
    "\n",
    "        return uni_id, region\n",
    "    \n",
    "    def _crawl_uni_contact(self, region: str, uni_id: str):\n",
    "\n",
    "        try:\n",
    "            contact_key = self.driver.find_element(By.XPATH, \"//h3[text()='Liên hệ']\")\n",
    "            contact_element = contact_key.find_element(By.XPATH, \"..\")\n",
    "\n",
    "            uni_contact_details = {}\n",
    "\n",
    "            try:\n",
    "                location_key = contact_element.find_element(By.XPATH, \"//strong[text()='Địa chỉ']\")\n",
    "                location_element = location_key.find_element(By.XPATH, \"..\")\n",
    "                location_string = location_element.find_element(By.TAG_NAME, \"p\").text\n",
    "            except Exception as e:\n",
    "                location_string = \"null\"\n",
    "                logging.warning(f\"[At {self._crawl_uni_contact.__name__} | Index: {self.progress} | {region} - {uni_id}]: Location not found.\")\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                phone_key = contact_element.find_element(By.XPATH, \"//*[@id='chitiettruong-tuyensinh']/div[1]/ul/li[2]/strong\")\n",
    "                phone_element = phone_key.find_element(By.XPATH, \"..\")\n",
    "                phone_string_elements = phone_element.find_elements(By.TAG_NAME, \"a\")\n",
    "                phone_list = []\n",
    "                for phone in phone_string_elements:\n",
    "                    phone_list.append(phone.text)\n",
    "            except Exception as e:\n",
    "                phone_list = \"null\"\n",
    "                logging.warning(f\"[At {self._crawl_uni_contact.__name__} | Index: {self.progress} | {region} - {uni_id}]: Phone not found.\")\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                website_key = contact_element.find_element(By.XPATH, \"//strong[text()='Website']\")\n",
    "                website_element = website_key.find_element(By.XPATH, \"..\")\n",
    "                website_string = website_element.find_element(By.TAG_NAME, \"p\").text\n",
    "            except Exception as e:\n",
    "                website_string = \"null\"\n",
    "                logging.warning(f\"[At {self._crawl_uni_contact.__name__} | Index: {self.progress} | {region} - {uni_id}]: Website not found.\")\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                email_key = contact_element.find_element(By.XPATH, \"//strong[text()='E-mail']\")\n",
    "                email_element = email_key.find_element(By.XPATH, \"..\")\n",
    "                email_strings = email_element.find_elements(By.TAG_NAME, \"a\")\n",
    "                email_list = []\n",
    "                for email in email_strings:\n",
    "                    email_list.append(email.text)\n",
    "            except Exception as e:\n",
    "                email_list = \"null\"\n",
    "                logging.warning(f\"[At {self._crawl_uni_contact.__name__} | Index: {self.progress} | {region} - {uni_id}]: Email not found.\")\n",
    "                pass\n",
    "\n",
    "            uni_contact_details[\"location\"] = location_string\n",
    "            uni_contact_details[\"phone\"] = phone_list\n",
    "            uni_contact_details[\"website\"] = website_string\n",
    "            uni_contact_details[\"email\"] = email_list\n",
    "\n",
    "            return uni_contact_details\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[At {self._crawl_uni_contact.__name__} | Index: {self.progress} | {region} - {uni_id}]: Failed to extract contact details.\")\n",
    "\n",
    "    def _format_number_string(self, number_string) -> int:\n",
    "        number_string = number_string.replace(\".\", \"\")\n",
    "        number_string = number_string.replace(\",\", \"\")\n",
    "        return int(number_string)\n",
    "    \n",
    "    def _get_year_from_string(self, string: str) -> int:\n",
    "        for i in string.split():\n",
    "            if i.isdigit():\n",
    "                if len(i) == 4:\n",
    "                    return int(i)\n",
    "\n",
    "    def _crawl_uni_admission_details(self, region: str, uni_id: str):\n",
    "\n",
    "        try:\n",
    "            admission_details_key = self.driver.find_element(By.XPATH, \"//h3[text()='Phương thức tuyển sinh năm 2024']\")\n",
    "            admission_details_element = admission_details_key.find_element(By.XPATH, \"..\")\n",
    "\n",
    "            admission_total_element = admission_details_element.find_element(By.XPATH, \"//p[@class='university__method-total']\")\n",
    "            admission_total_string = admission_total_element.text\n",
    "            admission_total_value = self._format_number_string(str(admission_total_string.split(\":\")[1].strip()))\n",
    "\n",
    "            admission_methods_key = admission_details_element.find_elements(By.XPATH, \"//li[@class='university__method-item']\")\n",
    "            admission_methods = []\n",
    "            for method in admission_methods_key:\n",
    "                admission_methods.append(method.text)\n",
    "\n",
    "            admission_details = {}\n",
    "            admission_details[\"year\"] = self._get_year_from_string(admission_details_key.text)\n",
    "            admission_details[\"admission_target\"] = admission_total_value\n",
    "            admission_details[\"methods\"] = admission_methods\n",
    "\n",
    "            return admission_details\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[At {self._crawl_uni_admission_details.__name__} | Index: {self.progress} | {region} - {uni_id}]: Admission Details not found.\")\n",
    "            admission_details = \"null\"\n",
    "            pass\n",
    "        \n",
    "    def _save_uni_general_info(self, name: str, id: str, contact_details: str, admission_details: str, region: str):\n",
    "        general_info_dict = {}\n",
    "        save_path = os.path.join(self.data_path, region, id)\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        try:\n",
    "            general_info_dict[\"name\"] = name\n",
    "            general_info_dict[\"id\"] = id\n",
    "            general_info_dict[\"region\"] = region\n",
    "            general_info_dict[\"contact_details\"] = contact_details\n",
    "            general_info_dict[\"admission_details\"] = admission_details\n",
    "\n",
    "            file_name = f\"{id}_info.json\"\n",
    "\n",
    "            logging.info(f\"Index {self.progress} | Saving '{region} - {id}' general information to '{save_path}' ...\")\n",
    "\n",
    "            with open(os.path.join(save_path, file_name), \"w\", encoding='utf-8') as f:\n",
    "                json.dump(general_info_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    def _crawl_uni_details(self):\n",
    "\n",
    "        uni_name = self.driver.find_element(By.CLASS_NAME, \"university__header-title\").text\n",
    "\n",
    "        uni_id, region = self._get_uni_id_and_region()\n",
    "\n",
    "        uni_contact = self._crawl_uni_contact(uni_id=uni_id, region=region)\n",
    "\n",
    "        uni_admission_detail = self._crawl_uni_admission_details(uni_id=uni_id, region=region)\n",
    "\n",
    "        self._save_uni_general_info(name=uni_name, id=uni_id, contact_details=uni_contact, admission_details=uni_admission_detail, region=region)\n",
    "\n",
    "        change_year_btn = self.driver.find_element(By.CLASS_NAME, \"select2-selection--single\")\n",
    "\n",
    "        for year in range(2024, 2014, -1):\n",
    "\n",
    "            ActionChains(self.driver).move_to_element(change_year_btn).click(change_year_btn).perform()\n",
    "\n",
    "            year_option = WebDriverWait(self.driver, 5).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, f\"//li[text()='Năm {year}']\"))\n",
    "            )\n",
    "\n",
    "            ActionChains(self.driver).click(year_option).pause(2).perform()\n",
    "\n",
    "            df = self._extract_table_to_df(uni_id=uni_id, region=region, year=year)\n",
    "\n",
    "            file_name = f\"{uni_id}_{year}.csv\"\n",
    "\n",
    "            self._save_dataframe(dataframe=df, file_name=file_name, uni_id=uni_id, region=region, year=year)\n",
    "\n",
    "    def quit(self):\n",
    "        self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROGRESS_TRACK = 34\n",
    "DATA_PATH  = \"data\"\n",
    "\n",
    "crawler = VnExpressUniCrawler(progress_track=PROGRESS_TRACK, data_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://diemthi.vnexpress.net/tra-cuu-dai-hoc/dai-hoc-hoa-sen-516\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(DATA_PATH, \"uni_links.txt\"), \"r\", encoding='utf-8') as f:\n",
    "    links = f.readlines()\n",
    "\n",
    "links[107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 13:58:45,057 - WARNING]: [At _crawl_uni_contact | Index: 307 | Khánh Hòa - TCU]: Email not found.\n",
      "[2024-09-16 13:58:45,065 - WARNING]: [At _crawl_uni_admission_details | Index: 307 | Khánh Hòa - TCU]: Admission Details not found.\n",
      "[2024-09-16 13:58:45,066 - INFO]: Index 307 | Saving 'Khánh Hòa - TCU' general information to 'data\\Khánh Hòa\\TCU' ...\n",
      "[2024-09-16 13:58:50,745 - WARNING]: [At iterate_through_uni_links]: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7A5929412+29090]\n",
      "\t(No symbol) [0x00007FF7A589E239]\n",
      "\t(No symbol) [0x00007FF7A575B1DA]\n",
      "\t(No symbol) [0x00007FF7A57AEFE7]\n",
      "\t(No symbol) [0x00007FF7A57AF23C]\n",
      "\t(No symbol) [0x00007FF7A57F97C7]\n",
      "\t(No symbol) [0x00007FF7A57D672F]\n",
      "\t(No symbol) [0x00007FF7A57F65A2]\n",
      "\t(No symbol) [0x00007FF7A57D6493]\n",
      "\t(No symbol) [0x00007FF7A57A09D1]\n",
      "\t(No symbol) [0x00007FF7A57A1B31]\n",
      "\tGetHandleVerifier [0x00007FF7A5C4871D+3302573]\n",
      "\tGetHandleVerifier [0x00007FF7A5C94243+3612627]\n",
      "\tGetHandleVerifier [0x00007FF7A5C8A417+3572135]\n",
      "\tGetHandleVerifier [0x00007FF7A59E5EB6+801862]\n",
      "\t(No symbol) [0x00007FF7A58A945F]\n",
      "\t(No symbol) [0x00007FF7A58A4FB4]\n",
      "\t(No symbol) [0x00007FF7A58A5140]\n",
      "\t(No symbol) [0x00007FF7A589461F]\n",
      "\tBaseThreadInitThunk [0x00007FFDBC68257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFDBD3CAF28+40]\n",
      "\n",
      "[2024-09-16 13:58:55,981 - INFO]: Restarting at Index 307: https://diemthi.vnexpress.net/tra-cuu-dai-hoc/truong-si-quan-thong-tin-he-dan-su-dai-hoc-thong-tin-lien-lac-329\n",
      "\n",
      "[2024-09-16 13:59:25,781 - WARNING]: [At _crawl_uni_contact | Index: 307 | Khánh Hòa - TCU]: Email not found.\n",
      "[2024-09-16 13:59:25,790 - WARNING]: [At _crawl_uni_admission_details | Index: 307 | Khánh Hòa - TCU]: Admission Details not found.\n",
      "[2024-09-16 13:59:25,791 - INFO]: Index 307 | Saving 'Khánh Hòa - TCU' general information to 'data\\Khánh Hòa\\TCU' ...\n",
      "[2024-09-16 13:59:28,705 - INFO]: Index 307 | Saving 'Khánh Hòa - TCU - 2024' admission scores to 'data\\Khánh Hòa\\TCU\\TCU_2024.csv' ...\n",
      "[2024-09-16 13:59:28,705 - WARNING]: [At _save_dataframe | Index 307 | Khánh Hòa - TCU - 2024]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 13:59:31,552 - INFO]: Index 307 | Saving 'Khánh Hòa - TCU - 2023' admission scores to 'data\\Khánh Hòa\\TCU\\TCU_2023.csv' ...\n",
      "[2024-09-16 13:59:31,552 - WARNING]: [At _save_dataframe | Index 307 | Khánh Hòa - TCU - 2023]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 13:59:34,398 - INFO]: Index 307 | Saving 'Khánh Hòa - TCU - 2022' admission scores to 'data\\Khánh Hòa\\TCU\\TCU_2022.csv' ...\n",
      "[2024-09-16 13:59:34,400 - WARNING]: [At _save_dataframe | Index 307 | Khánh Hòa - TCU - 2022]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 13:59:37,274 - INFO]: Index 307 | Saving 'Khánh Hòa - TCU - 2021' admission scores to 'data\\Khánh Hòa\\TCU\\TCU_2021.csv' ...\n",
      "[2024-09-16 13:59:37,274 - WARNING]: [At _save_dataframe | Index 307 | Khánh Hòa - TCU - 2021]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 13:59:40,138 - INFO]: Index 307 | Saving 'Khánh Hòa - TCU - 2020' admission scores to 'data\\Khánh Hòa\\TCU\\TCU_2020.csv' ...\n",
      "[2024-09-16 13:59:40,140 - WARNING]: [At _save_dataframe | Index 307 | Khánh Hòa - TCU - 2020]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 13:59:42,977 - INFO]: Index 307 | Saving 'Khánh Hòa - TCU - 2019' admission scores to 'data\\Khánh Hòa\\TCU\\TCU_2019.csv' ...\n",
      "[2024-09-16 13:59:42,978 - WARNING]: [At _save_dataframe | Index 307 | Khánh Hòa - TCU - 2019]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 13:59:45,812 - INFO]: Index 307 | Saving 'Khánh Hòa - TCU - 2018' admission scores to 'data\\Khánh Hòa\\TCU\\TCU_2018.csv' ...\n",
      "[2024-09-16 13:59:45,813 - WARNING]: [At _save_dataframe | Index 307 | Khánh Hòa - TCU - 2018]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 13:59:48,672 - INFO]: Index 307 | Saving 'Khánh Hòa - TCU - 2017' admission scores to 'data\\Khánh Hòa\\TCU\\TCU_2017.csv' ...\n",
      "[2024-09-16 13:59:51,509 - INFO]: Index 307 | Saving 'Khánh Hòa - TCU - 2016' admission scores to 'data\\Khánh Hòa\\TCU\\TCU_2016.csv' ...\n",
      "[2024-09-16 13:59:54,373 - INFO]: Index 307 | Saving 'Khánh Hòa - TCU - 2015' admission scores to 'data\\Khánh Hòa\\TCU\\TCU_2015.csv' ...\n",
      "[2024-09-16 13:59:54,374 - WARNING]: [At _save_dataframe | Index 307 | Khánh Hòa - TCU - 2015]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 13:59:56,206 - ERROR]: [At _crawl_uni_contact | Index: 307 | Khánh Hòa - TTH]: Failed to extract contact details.\n",
      "[2024-09-16 13:59:56,212 - WARNING]: [At _crawl_uni_admission_details | Index: 307 | Khánh Hòa - TTH]: Admission Details not found.\n",
      "[2024-09-16 13:59:56,213 - INFO]: Index 307 | Saving 'Khánh Hòa - TTH' general information to 'data\\Khánh Hòa\\TTH' ...\n",
      "[2024-09-16 13:59:59,097 - INFO]: Index 307 | Saving 'Khánh Hòa - TTH - 2024' admission scores to 'data\\Khánh Hòa\\TTH\\TTH_2024.csv' ...\n",
      "[2024-09-16 14:00:01,955 - INFO]: Index 307 | Saving 'Khánh Hòa - TTH - 2023' admission scores to 'data\\Khánh Hòa\\TTH\\TTH_2023.csv' ...\n",
      "[2024-09-16 14:00:04,807 - INFO]: Index 307 | Saving 'Khánh Hòa - TTH - 2022' admission scores to 'data\\Khánh Hòa\\TTH\\TTH_2022.csv' ...\n",
      "[2024-09-16 14:00:07,647 - INFO]: Index 307 | Saving 'Khánh Hòa - TTH - 2021' admission scores to 'data\\Khánh Hòa\\TTH\\TTH_2021.csv' ...\n",
      "[2024-09-16 14:00:10,488 - INFO]: Index 307 | Saving 'Khánh Hòa - TTH - 2020' admission scores to 'data\\Khánh Hòa\\TTH\\TTH_2020.csv' ...\n",
      "[2024-09-16 14:00:13,328 - INFO]: Index 307 | Saving 'Khánh Hòa - TTH - 2019' admission scores to 'data\\Khánh Hòa\\TTH\\TTH_2019.csv' ...\n",
      "[2024-09-16 14:00:16,172 - INFO]: Index 307 | Saving 'Khánh Hòa - TTH - 2018' admission scores to 'data\\Khánh Hòa\\TTH\\TTH_2018.csv' ...\n",
      "[2024-09-16 14:00:19,009 - INFO]: Index 307 | Saving 'Khánh Hòa - TTH - 2017' admission scores to 'data\\Khánh Hòa\\TTH\\TTH_2017.csv' ...\n",
      "[2024-09-16 14:00:21,856 - INFO]: Index 307 | Saving 'Khánh Hòa - TTH - 2016' admission scores to 'data\\Khánh Hòa\\TTH\\TTH_2016.csv' ...\n",
      "[2024-09-16 14:00:24,697 - INFO]: Index 307 | Saving 'Khánh Hòa - TTH - 2015' admission scores to 'data\\Khánh Hòa\\TTH\\TTH_2015.csv' ...\n",
      "[2024-09-16 14:00:27,081 - WARNING]: [At _crawl_uni_contact | Index: 308 | TP HCM - VPH]: Email not found.\n",
      "[2024-09-16 14:00:27,086 - WARNING]: [At _crawl_uni_admission_details | Index: 308 | TP HCM - VPH]: Admission Details not found.\n",
      "[2024-09-16 14:00:27,087 - INFO]: Index 308 | Saving 'TP HCM - VPH' general information to 'data\\TP HCM\\VPH' ...\n",
      "[2024-09-16 14:00:29,935 - INFO]: Index 308 | Saving 'TP HCM - VPH - 2024' admission scores to 'data\\TP HCM\\VPH\\VPH_2024.csv' ...\n",
      "[2024-09-16 14:00:29,936 - WARNING]: [At _save_dataframe | Index 308 | TP HCM - VPH - 2024]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 14:00:32,790 - INFO]: Index 308 | Saving 'TP HCM - VPH - 2023' admission scores to 'data\\TP HCM\\VPH\\VPH_2023.csv' ...\n",
      "[2024-09-16 14:00:32,790 - WARNING]: [At _save_dataframe | Index 308 | TP HCM - VPH - 2023]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 14:00:35,623 - INFO]: Index 308 | Saving 'TP HCM - VPH - 2022' admission scores to 'data\\TP HCM\\VPH\\VPH_2022.csv' ...\n",
      "[2024-09-16 14:00:35,623 - WARNING]: [At _save_dataframe | Index 308 | TP HCM - VPH - 2022]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 14:00:38,460 - INFO]: Index 308 | Saving 'TP HCM - VPH - 2021' admission scores to 'data\\TP HCM\\VPH\\VPH_2021.csv' ...\n",
      "[2024-09-16 14:00:38,460 - WARNING]: [At _save_dataframe | Index 308 | TP HCM - VPH - 2021]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 14:00:41,302 - INFO]: Index 308 | Saving 'TP HCM - VPH - 2020' admission scores to 'data\\TP HCM\\VPH\\VPH_2020.csv' ...\n",
      "[2024-09-16 14:00:41,302 - WARNING]: [At _save_dataframe | Index 308 | TP HCM - VPH - 2020]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 14:00:44,157 - INFO]: Index 308 | Saving 'TP HCM - VPH - 2019' admission scores to 'data\\TP HCM\\VPH\\VPH_2019.csv' ...\n",
      "[2024-09-16 14:00:47,011 - INFO]: Index 308 | Saving 'TP HCM - VPH - 2018' admission scores to 'data\\TP HCM\\VPH\\VPH_2018.csv' ...\n",
      "[2024-09-16 14:00:49,868 - INFO]: Index 308 | Saving 'TP HCM - VPH - 2017' admission scores to 'data\\TP HCM\\VPH\\VPH_2017.csv' ...\n",
      "[2024-09-16 14:00:52,706 - INFO]: Index 308 | Saving 'TP HCM - VPH - 2016' admission scores to 'data\\TP HCM\\VPH\\VPH_2016.csv' ...\n",
      "[2024-09-16 14:00:55,549 - INFO]: Index 308 | Saving 'TP HCM - VPH - 2015' admission scores to 'data\\TP HCM\\VPH\\VPH_2015.csv' ...\n",
      "[2024-09-16 14:00:57,354 - WARNING]: [At _crawl_uni_contact | Index: 309 | Đà Nẵng - DDV]: Email not found.\n",
      "[2024-09-16 14:00:57,361 - WARNING]: [At _crawl_uni_admission_details | Index: 309 | Đà Nẵng - DDV]: Admission Details not found.\n",
      "[2024-09-16 14:00:57,362 - INFO]: Index 309 | Saving 'Đà Nẵng - DDV' general information to 'data\\Đà Nẵng\\DDV' ...\n",
      "[2024-09-16 14:01:00,248 - INFO]: Index 309 | Saving 'Đà Nẵng - DDV - 2024' admission scores to 'data\\Đà Nẵng\\DDV\\DDV_2024.csv' ...\n",
      "[2024-09-16 14:01:03,126 - INFO]: Index 309 | Saving 'Đà Nẵng - DDV - 2023' admission scores to 'data\\Đà Nẵng\\DDV\\DDV_2023.csv' ...\n",
      "[2024-09-16 14:01:06,001 - INFO]: Index 309 | Saving 'Đà Nẵng - DDV - 2022' admission scores to 'data\\Đà Nẵng\\DDV\\DDV_2022.csv' ...\n",
      "[2024-09-16 14:01:08,895 - INFO]: Index 309 | Saving 'Đà Nẵng - DDV - 2021' admission scores to 'data\\Đà Nẵng\\DDV\\DDV_2021.csv' ...\n",
      "[2024-09-16 14:01:11,742 - INFO]: Index 309 | Saving 'Đà Nẵng - DDV - 2020' admission scores to 'data\\Đà Nẵng\\DDV\\DDV_2020.csv' ...\n",
      "[2024-09-16 14:01:14,609 - INFO]: Index 309 | Saving 'Đà Nẵng - DDV - 2019' admission scores to 'data\\Đà Nẵng\\DDV\\DDV_2019.csv' ...\n",
      "[2024-09-16 14:01:17,468 - INFO]: Index 309 | Saving 'Đà Nẵng - DDV - 2018' admission scores to 'data\\Đà Nẵng\\DDV\\DDV_2018.csv' ...\n",
      "[2024-09-16 14:01:20,298 - INFO]: Index 309 | Saving 'Đà Nẵng - DDV - 2017' admission scores to 'data\\Đà Nẵng\\DDV\\DDV_2017.csv' ...\n",
      "[2024-09-16 14:01:20,298 - WARNING]: [At _save_dataframe | Index 309 | Đà Nẵng - DDV - 2017]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 14:01:23,144 - INFO]: Index 309 | Saving 'Đà Nẵng - DDV - 2016' admission scores to 'data\\Đà Nẵng\\DDV\\DDV_2016.csv' ...\n",
      "[2024-09-16 14:01:23,145 - WARNING]: [At _save_dataframe | Index 309 | Đà Nẵng - DDV - 2016]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-16 14:01:26,000 - INFO]: Index 309 | Saving 'Đà Nẵng - DDV - 2015' admission scores to 'data\\Đà Nẵng\\DDV\\DDV_2015.csv' ...\n",
      "[2024-09-16 14:01:26,000 - WARNING]: [At _save_dataframe | Index 309 | Đà Nẵng - DDV - 2015]: No dataframe found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No tables found\n"
     ]
    }
   ],
   "source": [
    "crawler.iterate_through_uni_links(index=307, links=links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
